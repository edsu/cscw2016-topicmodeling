Crowdsourcing is a common strategy for collecting the “gold standard” labels required for many natural language applations. Crowdworkers differ in their responses for many reasons, but existing approaches often treat disagreements as “noise” to be removed through ﬁltering or aggregation. In this paper, we introduce the workﬂow design pattern of crowd parting: separating workers based on shared patterns in responses to a crowdsourcing task. We illustrate this idea using an automated clustering-based method to identify diveent, but valid, worker interpretations in crowdsourced entity annotations collected over two distinct corpora – Wikipedia articles and Tweets. We demonstrate how the intermediatevel view provide by crowd-parting analysis provides insight into sources of disagreement not easily gleaned from viewing either individual annotation sets or aggregated results. We diuss several concrete applications for how this approach could be applied directly to improving the quality and efﬁciency of crowdsourced annotation tasks. 