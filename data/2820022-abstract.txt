Social media gives voice to the people, but also opens the door to low-quality contributions, which degrade the expernce for the majority of users. To address the latter issue, the prevailing solution is to rely on the ”wisdom of the crowds” to promote good content (e.g., via votes or ”like” buttons), or to downgrade bad content. Unfortunately, such crowd feedback may be sparse, subjective, and slow to accumulate. In this per, we investigate the effects, on the users, of automatically ﬁltering question-answering content, using a combination of syntactic, semantic, and social signals. Using this ﬁltering, a large-scale experiment with real users was performed to meure the resulting engagement and satisfaction. To our knowdge, this experiment represents the ﬁrst reported large-scale user study of automatically curating social media content in real time. Our results show that automated quality ﬁltering indeed improves user engagement, usually aligning with, and often outperforming, crowd-based quality judgments. 